---
title: "Regressão e Vinho com Caret"
author: "Nazareno Andrade e João Arthur B. Monteiro"
date: "26 de julho de 2015"
output: html_document
---

```{r}
require(ggplot2)
require(caret)
require(GGally)
require(plyr)
require(dplyr)
```

# Preliminares

Usaremos dados de vinhos: http://www3.dsi.uminho.pt/pcortez/wine/

```{r}
wines <- read.csv("winequality/winequality-red.csv", sep=";")
summary(wines)

if(! file.exists("ggpairs.pdf")){
  pdf("ggpairs.pdf", w = 15, h = 15)
  ggpairs(wines, alpha=0.3)
  dev.off()
}

# do caret:
featurePlot(wines[,1:11], wines[,12])
```

Treino e teste

```{r}
split<-createDataPartition(y = wines$quality, 
                           p = 0.7, 
                           list = FALSE)

wines.dev <- wines[split,]
wines.val <- wines[-split,]
```


# Regressão linear

As pressuposições de regressão para previsão são: 
- A relação entre preditores e variável de resposta é aproximadamente linear
- Nenhum preditor pode ser derivado por combinação linear dos demais
- Há um número igual ou maior de observações do que preditores

Preditores com muita correlação entre si (aka colinearidade) também geram instabilidade no modelo. Para eliminar preditores com alta colinearidade:


```{r}
correlationMatrix <- cor(wines[,1:11])
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
# se houvesse, faríamos: 
# wines.filtered <- wines[,-highlyCorrelated]
```

Caso haja relações não-lineares entre preditores e resposta, podemos adicionar preditores transformados como fatores extra. 

Outro ponto importante: outliers influenciam muito a regressão linear. (Por causa do algoritmo de mínimos quadrados.)

### Treinando

Não há parâmetro para ajustar na regressão linear simples.

```{r}
ctrl <- trainControl(method = "cv", number = 10)

lmFit <- train(quality ~. , 
               data = wines.dev, 
               method = "lm", 
               trControl = ctrl,
               metric = "RMSE")

lmFit

summary(lmFit)
```

### Diagnósticos do modelo

```{r}
plot(varImp(lmFit))

# Usando os dados de treino!

avaliacao <- data.frame(obs = wines.dev$quality, pred = predict(lmFit), res = resid(lmFit))

ggplot(avaliacao, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  stat_abline(colour = "blue") + 
  ggtitle("Observado x Previsão (validação)")

ggplot(avaliacao, aes(y = res, x = pred)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.1)) + 
  geom_abline(slope = 0, intercept = 0, colour = "darkred") + 
  ggtitle("Resíduos na validação")

# Outra forma de fazer:
#
# xyplot(wines.dev$quality ~ predict(lmFit),
#        ## plot the points (type = 'p') and a background grid ('g')
#        type = c("p", "g"),
#        xlab = "Predicted", ylab = "Observed")
# 
# xyplot(resid(lmFit) ~ predict(lmFit),
#        type = c("p", "g"),
#        xlab = "Predicted", ylab = "Residuals")



```

Desempenho:

```{r}
predictedVal <- predict(lmFit, wines.val)
modelvalues<-data.frame(obs = wines.val$quality, pred = predictedVal)

defaultSummary(modelvalues)

compare <- data.frame(obs = wines.val$quality, pred = predictedVal)
ggplot(compare, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  stat_abline() + 
  ggtitle("Observado x Previsão (validação)")

ggplot(compare, aes(y = (pred - obs), x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  ggtitle("Resíduos na validação")

```

------

# Outro tipo de regressão

Usaremos MARS

```{r}
require(earth)
```

Sem model tuning:

```{r}
marsSimpleFit <- earth(quality ~ ., data = wines.dev) 
marsSimpleFit
summary(marsSimpleFit)
plotmo(marsSimpleFit)
```

Agora com caret. Há dois parâmetros para tuning: grau do modelo (quantas variáveis podem interagir em um termo da equação?) e número de termos no modelo final. 

```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:40)

marsFit <- train(quality ~. , 
                 data = wines.dev, 
                 method = "earth", 
                 trControl = ctrl,
                 # novidade:
                 tuneGrid = marsGrid,
                 metric = "RMSE")

marsFit
```

Diagnóstico

```{r}
plot(varImp(marsFit))

# Usando os dados de treino!
avaliacao$modelo <- "RL"

pred_mars <- data.frame(obs = wines.dev$quality, 
                        pred = predict(marsFit), 
                        res = wines.dev$quality - predict(marsFit),
                        modelo = "MARS")
avaliacao <- rbind(avaliacao, pred_mars)

ggplot(avaliacao, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  stat_abline(colour = "blue") + 
  facet_grid(. ~ modelo) + 
  ggtitle("Observado x Previsão (validação)")

ggplot(avaliacao, aes(y = res, x = pred)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.1)) + 
  geom_abline(slope = 0, intercept = 0, colour = "darkred") + 
  facet_grid(. ~ modelo) + 
  ggtitle("Resíduos na validação")

```

O desempenho melhorou?

```{r}
predictedVal <- predict(marsFit, wines.val)
modelvalues<-data.frame(obs = wines.val$quality, pred = predictedVal)

defaultSummary(modelvalues)

compare$modelo <- "RL" 
pred_mars <- data.frame(obs = wines.val$quality, pred = predictedVal, modelo = "MARS")
compare <- rbind(compare, pred_mars)

ggplot(compare, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  stat_abline() +
  ggtitle("Observado x Previsão (validação)")

ggplot(compare, aes(y = (pred - obs), x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  geom_abline(slope = 0, intercept = 0, colour = "darkred") + 
  ggtitle("Resíduos na validação")

```

## Bagged MARS 

MARS + Bootstrap AGGregation

```{r}
bMarsFit <- train(quality ~. , 
                 data = wines.dev, 
                 method = "bagEarthGCV", 
                 trControl = ctrl,
                 tuneGrid = expand.grid(.degree = 1:2),
                 metric = "RMSE")

bMarsFit

predictedVal <- predict(bMarsFit, wines.val)
modelvalues<-data.frame(obs = wines.val$quality, pred = predictedVal)

defaultSummary(modelvalues)
```

## Boosted LM

```{r}
bstLsFit <- train(quality ~. , 
                 data = wines.dev, 
                 method = "bstLs", 
                 trControl = ctrl,
                 metric = "RMSE")

bstLsFit
summary(bstLsFit)

predictedVal <- predict(bstLsFit, wines.val)
modelvalues<-data.frame(obs = wines.val$quality, pred = predictedVal)

defaultSummary(modelvalues)

pred_blm <- data.frame(obs = wines.val$quality, pred = predictedVal, modelo = "Boosted LM")
compare <- rbind(compare, pred_blm)

ggplot(compare, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  stat_abline() +
  ggtitle("Observado x Previsão (validação)")

ggplot(compare, aes(y = (pred - obs), x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  geom_abline(slope = 0, intercept = 0, colour = "darkred") + 
  ggtitle("Resíduos na validação")

```


# kNN 

```{r}
knnFit <- train(quality ~. , 
                data = wines.dev, 
                method = "knn", 
                trControl = ctrl,
                preProcess = c("center","scale"), 
                tuneGrid = expand.grid(.k = 3:6),
                metric = "RMSE")

knnFit

predictedVal <- predict(knnFit, wines.val)
modelvalues<-data.frame(obs = wines.val$quality, pred = predictedVal)

defaultSummary(modelvalues)

pred_knn <- data.frame(obs = wines.val$quality, pred = predictedVal, modelo = "kNN")
compare <- rbind(compare, pred_knn)

ggplot(compare, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  stat_abline(colour = "darkblue") +
  #ylim(3, 8) + 
  ggtitle("Observado x Previsão (validação)")

ggplot(compare, aes(y = (pred - obs), x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  geom_abline(slope = 0, intercept = 0, colour = "darkred") + 
  ggtitle("Resíduos na validação")

```

# Cubist

```{r}
cubFit <- train(quality ~. , 
                data = wines.dev, 
                method = "cubist", 
                trControl = ctrl,
                metric = "RMSE")

cubFit

predictedVal <- predict(cubFit, wines.val)
modelvalues<-data.frame(obs = wines.val$quality, pred = predictedVal)

defaultSummary(modelvalues)

pred_cub <- data.frame(obs = wines.val$quality, pred = predictedVal, modelo = "Cubist")
compare <- rbind(compare, pred_cub)

ggplot(compare, aes(y = pred, x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  stat_abline(colour = "darkblue") +
  #ylim(3, 8) + 
  ggtitle("Observado x Previsão (validação)")

ggplot(compare, aes(y = (pred - obs), x = obs)) + 
  geom_point(alpha = 0.5, position = position_jitter(width=0.2)) + 
  facet_grid(. ~ modelo) + 
  geom_abline(slope = 0, intercept = 0, colour = "darkred") + 
  ggtitle("Resíduos na validação")

plot(varImp(cubFit))
```

-----------

# Conselho de metodologia

Do livro de Kuhn (o autor do caret):

1. Start with several models that are the least interpretable and most flexible, such as boosted trees or support vector machines. Across many problem domains, these models have a high likelihood of producing the empirically optimum results (i.e., most accurate).

2. Investigate simpler models that are less opaque (e.g., not complete black boxes), such as multivariate adaptive regression splines (MARS), partial least squares, generalized additive models, or na ̈ıve Bayes models.

3. Consider using the simplest model that reasonably approximates the per- formance of the more complex methods.

Using this methodology, the modeler can discover the “performance ceiling” for the data set before settling on a model. In many cases, a range of models will be equivalent in terms of performance so the practitioner can weight the benefits of different methodologies (e.g., computational complexity, easy of prediction, interpretability). For example, a nonlinear support vector machine or random forest model might have superior accuracy, but the complexity and scope of the prediction equation may prohibit exporting the prediction equation to a production system. However, if a more interpretable model, such as a MARS model, yielded similar accuracy, the implementation of the prediction equation would be trivial and would also have superior execution time.